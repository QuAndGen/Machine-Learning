
missing_fixer <- function(na_value) {
  function(x) {
    x[x == na_value] <- NA
    x
  }
}
fix_missing_99 <- missing_fixer(-99)
fix_missing_999 <- missing_fixer(-999)
aaa<-missing_fixer(34534)
fix_missing_99(c(-99, -999))
fix_missing_999(c(-99, -999))

#Extra argument
fix_missing <- function(x, na.value) {
  x[x == na.value] <- NA
  x
}


summary <- function(x) {
  c(mean(x), median(x), sd(x), mad(x), IQR(x))
}
lapply(df, summary)

summary <- function(x) {
  c(mean(x, na.rm = TRUE),
    median(x, na.rm = TRUE),
    sd(x, na.rm = TRUE),
    mad(x, na.rm = TRUE),
    IQR(x, na.rm = TRUE))
}


summary <- function(x) {
  funs <- c(mean, median, sd, mad, IQR)
  lapply(funs, function(f) f(x, na.rm = TRUE))
}


# With appropriate parenthesis, the function is called:
(function(x) 3)()

# So this anonymous function syntax
(function(x) x + 3)(10)
# behaves exactly the same as
f <- function(x) x + 3
f(10)


#Closures

power <- function(exponent) {
  function(x) {
    x ^ exponent
  }
}

square <- power(2)
square(2)

cube <- power(3)
cube(2)
#When you print a closure, you don't see anything terribly useful:

#That's because the function itself doesn't change. The difference is the enclosing 
#environment, environment(square). One way to see the contents of the environment 
#is to convert it to a list:

as.list(environment(square))
as.list(environment(cube))

#The parent environment of a closure is the execution environment of the 
#function that created it, as shown by this code:


power <- function(exponent) {
  print(environment())
  function(x) x ^ exponent
}
zero <- power(0)

environment(zero)

#Mutable state

# The key to managing variables at different levels is the double arrow assignment 
#operator (<<-). Unlike the usual single arrow assignment (<-) that always assigns 
#in the current environment, the double arrow operator will keep looking up the chain 
#of parent environments until it finds a matching name. 
#Together, a static parent environment and <<- make it possible to maintain state across function calls.
new_counter <- function() {
  i <- 0
  function() {
    i <<- i + 1
    i
  }
}

counter_one <- new_counter()
counter_two <- new_counter()

counter_one()

counter_one()
counter_two()


#The counters get around the "fresh start" limitation by not modifying variables in their 
#local environment. Since the changes are made in the unchanging parent (or enclosing) environment, 
#they are preserved across function calls.

#What happens if you don't use a closure? What happens if you use <- instead of <<-? 
#Make predictions about what will happen if you replace new_counter() with the variants below, 
#then run the code and check your predictions.


i <- 0
new_counter2 <- function() {
  i <<- i + 1
  i
}
new_counter3 <- function() {
  i <- 0
  function() {
    i <- i + 1
    i
  }
}


#Lists of functions

compute_mean <- list(
  base = function(x) mean(x),
  sum = function(x) sum(x) / length(x),
  manual = function(x) {
    total <- 0
    n <- length(x)
    for (i in seq_along(x)) {
      total <- total + x[i] / n
    }
    total
  }
)

#Calling a function from a list is straightforward. You extract it then call it:

x <- runif(1e5)
system.time(compute_mean$base(x))

system.time(compute_mean[[2]](x))

system.time(compute_mean[["manual"]](x))


x <- 1:10
funs <- list(
  sum = sum,
  mean = mean,
  median = median
)
lapply(funs, function(f) f(x))

funs2 <- list(
  sum = function(x, ...) sum(x, ..., na.rm = TRUE),
  mean = function(x, ...) mean(x, ..., na.rm = TRUE),
  median = function(x, ...) median(x, ..., na.rm = TRUE)
)
lapply(funs2, function(f) f(x))

#Moving lists of functions to the global environment

#From time to time you may create a list of functions that you want to be available 
#without having to use a special syntax. For example, imagine you want to create 
#HTML code by mapping each tag to an R function. The following example uses a function 
#3factory to create functions for the tags <p> (paragraph), <b> (bold), and <i>


simple_tag <- function(tag) {
  force(tag)
  function(...) {
    paste0("<", tag, ">", paste0(...), "</", tag, ">")
  }
}
tags <- c("p", "b", "i")
html <- lapply(setNames(tags, tags), simple_tag)

#I've put the functions in a list because I don't want them to be available all the time. 
#The risk of a conflict between an existing R function and an HTML tag is high. 
#But keeping them in a list makes code more verbose:
 

html$p("This is ", html$b("bold"), " text.")

#Depending on how long we want the effect to last, you have three options to eliminate the use of html$:
#For a very temporary effect, you can use with():
with(html, p("This is ", b("bold"), " text."))
#For a longer effect, you can attach() the functions to the search path, then detach() when you're done:
attach(html)
p("This is ", b("bold"), " text.")

detach(html)

#Finally, you could copy the functions to the global environment with list2env(). 
#You can undo this by deleting the functions after you're done.

list2env(html, environment())

p("This is ", b("bold"), " text.")

rm(list = names(html), envir = environment())





#Functionals******************************************************************************

#A higher-order function is a function that takes a function as an input or returns a 
#function as output. We've already seen one type of higher order function: closures, 
#functions returned by another function. The complement to a closure is a functional, 
#a function that takes a function as an input and returns a vector as output. Here's a 
#simple functional: it calls the function provided as input with 1000 random uniform numbers.

randomise <- function(f) f(runif(1e3))
randomise(mean)

randomise(sum)

#The chances are that you've already used a functional: the three most 
#frequently used are lapply(), apply(), and tapply(). 

#lapply()

#lapply() is written in C for performance, but we can create a simple R implementation 
#that does the same thing:

lapply2 <- function(x, f, ...) {
  out <- vector("list", length(x))
  for (i in seq_along(x)) {
    out[[i]] <- f(x[[i]], ...)
  }
  out
}

#lapply() makes it easier to work with lists by eliminating much of the boilerplate 
#associated with looping. This allows you to focus on the function that you're applying:


# Create some random data
l <- replicate(20, runif(sample(1:10, 1)), simplify = FALSE)

# With a for loop
out <- vector("list", length(l))
for (i in seq_along(l)) {
  out[[i]] <- length(l[[i]])
}
unlist(out)

# With lapply
unlist(lapply(l, length))

# What class is each column?
unlist(lapply(mtcars, class))

# Divide each column by the mean
mtcars[] <- lapply(mtcars, function(x) x / mean(x))


trims <- c(0, 0.1, 0.2, 0.5)
x <- rcauchy(1000)
unlist(lapply(trims, function(trim) mean(x, trim = trim)))

#Looping patterns

# It's useful to remember that there are three basic ways to loop over a vector:
#   
#   loop over the elements: for (x in xs)
#   loop over the numeric indices: for (i in seq_along(xs))
#   loop over the names: for (nm in names(xs))
      

xs <- runif(1e3)
res <- c()
for (x in xs) {
  # This is slow!
  res <- c(res, sqrt(x))
}

#This is slow because each time you extend the vector, R has to copy all of the 
#existing elements. Avoid copies discusses this problem in more depth. Instead, 
#it's much better to create the space you'll need for the output and then fill it in. 
#This is easiest with the second form:
 
res <- numeric(length(xs))
for (i in seq_along(xs)) {
  res[i] <- sqrt(xs[i])
}

#Just as there are three basic ways to use a for loop, there are three basic ways to use lapply():

lapply(xs, function(x) {})
lapply(seq_along(xs), function(i) {})
lapply(names(xs), function(nm) {})

# The following sections build on lapply() and discuss:
#   
# sapply() and vapply(), variants of lapply() that produce vectors, matrices, 
# #and arrays as output, instead of lists.
# 
# Map() and mapply() which iterate over multiple input data structures in parallel.
# 
# mclapply() and mcMap(), parallel versions of lapply() and Map().
# 
# Writing a new function, rollapply(), to solve a new problem.


#Vector output: sapply and vapply

#sapply() and vapply() are very similar to lapply() except they simplify their 
#output to produce an atomic vector. While sapply() guesses, vapply() takes an 
#additional argument specifying the output type. 

sapply(mtcars, is.numeric)

vapply(mtcars, is.numeric, logical(1))

sapply(list(), is.numeric)
vapply(list(), is.numeric, logical(1))

#If the function returns results of different types or lengths, 
#sapply() will silently return a list, while vapply() will throw an error. 
#sapply() is fine for interactive use because you'll normally notice if 
#something goes wrong, but it's dangerous when writing functions.


df <- data.frame(x = 1:10, y = letters[1:10])
sapply(df, class)

vapply(df, class, character(1))

df2 <- data.frame(x = 1:10, y = Sys.time() + 1:10)
sapply(df2, class)

vapply(df2, class, character(1))

#sapply() is a thin wrapper around lapply() that transforms a list 
#into a vector in the final step. vapply() is an implementation of lapply() 
#that assigns results to a vector (or matrix) of appropriate type instead of as a list.

sapply2 <- function(x, f, ...) {
  res <- lapply2(x, f, ...)
  simplify2array(res)
}

vapply2 <- function(x, f, f.value, ...) {
  out <- matrix(rep(f.value, length(x)), nrow = length(f.value))
  for (i in seq_along(x)) {
    res <- f(x[[i]], ...)
    stopifnot(
      length(res) == length(f.value),
      typeof(res) == typeof(f.value)
    )
    out[ ,i] <- res
  }
  out
}

#Multiple inputs: Map (and mapply)------

#With lapply(), only one argument to the function varies; the others are fixed. 
#This makes it poorly suited for some problems. For example, how would you find 
#a weighted mean when you have two lists, one of observations and the other of weights?
  

# Generate some sample data
xs <- replicate(5, runif(10), simplify = FALSE)
ws <- replicate(5, rpois(10, 5) + 1, simplify = FALSE)

#It's easy to use lapply() to compute the unweighted means:
unlist(lapply(xs, mean))

#But how could we supply the weights to weighted.mean()? lapply(x, means, w) 
#won't work because the additional arguments to lapply() are passed to every call. 
#We could change looping forms:


unlist(lapply(seq_along(xs), function(i) {
  weighted.mean(xs[[i]], ws[[i]])
}))

#This works, but it's a little clumsy. A cleaner alternative is to use Map, 
#a variant of lapply(), where all arguments can vary. This lets us write:

unlist(Map(weighted.mean, xs, ws))

#Note that the order of arguments is a little different: function 
#is the first argument for Map() and the second for lapply()

stopifnot(length(xs) == length(ws))
out <- vector("list", length(xs))
for (i in seq_along(xs)) {
  out[[i]] <- weighted.mean(xs[[i]], ws[[i]])
}

#There's a natural equivalence between Map() and lapply() because you can 
#always convert a Map() to an lapply() that iterates over indices. 


#Map is useful whenever you have two (or more) lists (or data frames) that you need 
#to process in parallel. For example, another way of standardising columns is to 
#first compute the means and then divide by them. We could do this with lapply(), 
#but if we do it in two steps


mtmeans <- lapply(mtcars, mean)
mtmeans[] <- Map(`/`, mtcars, mtmeans)

# In this case, equivalent to
mtcars[] <- lapply(mtcars, function(x) x / mean(x))

#If some of the arguments should be fixed and constant, use an anonymous function:
Map(function(x, w) weighted.mean(x, w, na.rm = TRUE), xs, ws)

#mapply-------------------------------------------------------------

#You may be more familiar with mapply() than Map(). I prefer Map() because:

# It's equivalent to mapply with simplify = FALSE, which is almost always what you want.
# 
# Instead of using an anonymous function to provide constant inputs, mapply has the 
# #MoreArgs argument that takes a list of extra arguments that will be supplied, as is, 
# #to each call. This breaks R's usual lazy evaluation semantics, and is inconsistent 
# #with other functions.
# 
# In brief, mapply() adds more complication for little gain.


#Rolling computations

rollmean <- function(x, n) {
  out <- rep(NA, length(x))
  
  offset <- trunc(n / 2)
  for (i in (offset + 1):(length(x) - n + offset + 1)) {
    out[i] <- mean(x[(i - offset):(i + offset - 1)])
  }
  out
}
x <- seq(1, 3, length = 1e2) + runif(1e2)
plot(x)
lines(rollmean(x, 5), col = "blue", lwd = 2)
lines(rollmean(x, 10), col = "red", lwd = 2)

#To change rollmean() to rollmedian(), all you need to do is replace mean with 
#median inside the loop.

rollapply <- function(x, n, f, ...) {
  out <- rep(NA, length(x))
  
  offset <- trunc(n / 2)
  for (i in (offset + 1):(length(x) - n + offset + 1)) {
    out[i] <- f(x[(i - offset):(i + offset - 1)], ...)
  }
  out
}
plot(x)
lines(rollapply(x, 5, median), col = "red", lwd = 2)

#You might notice that the internal loop looks pretty similar to a vapply() loop, 
#so we could rewrite the function as:

rollapply <- function(x, n, f, ...) {
  offset <- trunc(n / 2)
  locs <- (offset + 1):(length(x) - n + offset + 1)
  num <- vapply(
    locs, 
    function(i) f(x[(i - offset):(i + offset)], ...),
    numeric(1)
  )
  
  c(rep(NA, offset), num)
}




#Parallelisation--------------------------------------------

#One interesting thing about the implementation of lapply() is that because each 
#iteration is isolated from all others, the order in which they are computed doesn't matter. 
#For example, lapply3() scrambles the order of computation, but the results are 
#always the same:

lapply3 <- function(x, f, ...) {
  out <- vector("list", length(x))
  for (i in sample(seq_along(x))) {
    out[[i]] <- f(x[[i]], ...)
  }
  out
}
unlist(lapply(1:10, sqrt))

#This has a very important consequence: since we can compute each element in any order, 
#it's easy to dispatch the tasks to different cores, and compute them in parallel. 
#This is what parallel::mclapply() (and parallel::mcMap()) does. 
#(These functions are not available in Windows, but you can use the similar parLapply() 
#with a bit more work. See parallelise for more details.)


library(parallel)
unlist(mclapply(1:10, sqrt, mc.cores = 4))

#In this case, mclapply() is actually slower than lapply(). This is because the cost of 
#the individual computations is low, and additional work is needed to send the computation 
#to the different cores and to collect the results.

#If we take a more realistic example, generating bootstrap replicates of a linear model 
#for example, the advantages are clearer:
  
boot_df <- function(x) x[sample(nrow(x), rep = T), ]
rsquared <- function(mod) summary(mod)$r.square
boot_lm <- function(i) {
  rsquared(lm(mpg ~ wt + disp, data = boot_df(mtcars)))
}

system.time(lapply(1:500, boot_lm))

system.time(mclapply(1:500, boot_lm, mc.cores = 2))

#While increasing the number of cores will not always lead to linear improvement, 
#switching from lapply() or Map() to its parallelised forms can dramatically 
#improve computational performance.

#Manipulating matrices and data frames--------------------------------------------

#Functionals can also be used to eliminate loops in common data manipulation tasks. 
#In this section, we'll give a brief overview of the available options, hint at how 
#they can help you, and point you in the right direction to learn more. 
#We'll cover three categories of data structure functionals:

# apply(), sweep(), and outer() work with matrices.
# 
# tapply() summarises a vector by groups defined by another vector.
# 
# the plyr package, which generalises tapply() to make it easy to work with data frames, 
# #lists, or arrays as inputs, and data frames, lists, or arrays as outputs.



# Matrix and array operations--------------------
# So far, all the functionals we've seen work with 1d input structures. The three functionals in this section provide useful tools for working with higher-dimensional data structures. apply() is a variant of sapply() that works with matrices and arrays. You can think of it as an operation that summarises a matrix or array by collapsing each row or column to a single number. It has four arguments:
#   
#   X, the matrix or array to summarise
#   MARGIN, an integer vector giving the dimensions to summarise over, 1 = rows, 2 = columns, etc.
#   FUN, a summary function
#   ... other arguments passed on to FUN
  
#A typical example of apply() looks like this

a <- matrix(1:20, nrow = 5)
apply(a, 1, mean)

apply(a, 2, mean)

#There are a few caveats to using apply(). It doesn't have a simplify argument, 
#so you can never be completely sure what type of output you'll get. 

a1 <- apply(a, 1, identity)
identical(a, a1)

identical(a, t(a1))

a2 <- apply(a, 2, identity)
identical(a, a2)

#sweep() allows you to "sweep" out the values of a summary statistic. It is 
#often used with apply() to standardise arrays. The following example scales 
#the rows of a matrix so that all values lie between 0 and 1.

x <- matrix(rnorm(20, 0, 10), nrow = 4)
x1 <- sweep(x, 1, apply(x, 1, min), `-`)
x2 <- sweep(x1, 1, apply(x1, 1, max), `/`)

#The final matrix functional is outer(). It's a little different in that 
#it takes multiple vector inputs and creates a matrix or array output where 
#the input function is run over every combination of the inputs:


# Create a times table
outer(1:3, 1:10, "*")


#Group apply-------------------------------

#You can think about tapply() as a generalisation to apply() that 
#allows for "ragged" arrays, arrays where each row can have a different 
#number of columns. This is often needed when you're trying to summarise a data set. 


pulse <- round(rnorm(22, 70, 10 / 3)) + rep(c(0, 5), c(10, 12))
group <- rep(c("A", "B"), c(10, 12))

tapply(pulse, group, length)

tapply(pulse, group, mean)

#tapply() works by creating a "ragged" data structure from a set of inputs, 
#and then applying a function to the individual elements of that structure. 
#The first task is actually what the split() function does. It takes two inputs 
#and returns a list which groups elements together from the first vector according 
#to elements, or categories, from the second vector:

split(pulse, group)

#Then tapply() is just the combination of split() and sapply():

tapply2 <- function(x, group, f, ..., simplify = TRUE) {
  pieces <- split(x, group)
  sapply(pieces, f, simplify = simplify)
}
tapply2(pulse, group, length)

tapply2(pulse, group, mean)

#Manipulating lists-------------------------------
# Another way of thinking about functionals is as a set of general tools for 
# altering, subsetting, and collapsing lists. Every functional programming language 
# has three tools for this: Map(), Reduce(), and Filter(). We've seen Map() already, 
# and the following sections describe Reduce(), a powerful tool for extending two-argument 
# functions, and Filter(), a member of an important class of functionals that work 
# with predicates, functions that return a single TRUE or FALSE.
# 


#Reduce()--------------------

#Reduce() reduces a vector, x, to a single value by recursively calling a function, 
#f, two arguments at a time. It combines the first two elements with f, then combines 
#the result of that call with the third element, and so on. Calling Reduce(f, 1:3) is 
#equivalent to f(f(1, 2), 3).


#The following two examples show what Reduce does with an infix and prefix function:

Reduce(`+`, 1:3) # -> ((1 + 2) + 3)
Reduce(sum, 1:3) # -> sum(sum(1, 2), 3)
#The essence of Reduce() can be described by a simple for loop:
Reduce2 <- function(f, x) {
  out <- x[[1]]
  for(i in seq(2, length(x))) {
    out <- f(out, x[[i]])
  }
  out
}

#Reduce() is an elegant way of extending a function that works with two 
#inputs into a function that can deal with any number of inputs. It's useful for 
#implementing many types of recursive operations, like merges and intersections.


l <- replicate(5, sample(1:10, 15, replace = T), simplify = FALSE)
str(l)

#You could do that by intersecting each element in turn:
 
intersect(intersect(intersect(intersect(l[[1]], l[[2]]),
                              l[[3]]), l[[4]]), l[[5]])

#That's hard to read. With Reduce(), the equivalent is:

Reduce(intersect, l)

#Predicate functionals
#A predicate is a function that returns a single TRUE or FALSE, like is.character, all, 
#or is.NULL. A predicate functional applies a predicate to each element of a list or 
#data frame. There are three useful predicate functionals in base R: Filter(), Find(), 
#and Position().

#Filter() selects only those elements which match the predicate.

#Find() returns the first element which matches the predicate 
#(or the last element if right = TRUE).

#Position() returns the position of the first element that matches the predicate 
#(or the last element if right = TRUE).

#Another useful predicate functional is where(), a custom functional that generates a 
#logical vector from a list (or a data frame) and a predicate:
  
where <- function(f, x) {
  vapply(x, f, logical(1))
}

#The following example shows how you might use these functionals with a data frame:

df <- data.frame(x = 1:3, y = c("a", "b", "c"))
where(is.factor, df)

str(Filter(is.factor, df))

data.frame(Filter(is.numeric,mtcars))

str(Find(is.factor, df))

Position(is.factor, df)

#Mathematical functionals-----------------------------------------------

#Functionals are very common in mathematics. The limit, the maximum, the roots 
#(the set of points where f(x) = 0), and the definite integral are all functionals: 
#given a function, they return a single number (or vector of numbers).


#There are three functionals that work with functions to return single numeric values:
  
# integrate() finds the area under the curve defined by f()
# uniroot() finds where f() hits zero
# optimise() finds the location of lowest (or highest) value of f()

# Let's explore how these are used with a simple function, sin():
integrate(sin, 0, pi)
str(uniroot(sin, pi * c(1 / 2, 3 / 2)))
str(optimise(sin, c(0, 2 * pi)))
str(optimise(sin, c(0, pi), maximum = TRUE))

#The following example shows how we might find the maximum likelihood estimate for ??, 
#if our data come from a Poisson distribution. First, we create a function factory that, 
#given a dataset, returns a function that computes the negative log likelihood (NLL) for 
#parameter lambda. In R, it's common to work with the negative since optimise() defaults 
#to finding the minimum.

poisson_nll <- function(x) {
  n <- length(x)
  sum_x <- sum(x)
  function(lambda) {
    n * lambda - sum_x * log(lambda) # + terms not involving lambda
  }
}

#We can use this function factory to generate specific NLL functions for input data. 
#Then optimise() allows us to find the best values (the maximum likelihood estimates), 
#given a generous starting range.


x1 <- c(41, 30, 31, 38, 29, 24, 30, 29, 31, 38)
x2 <- c(6, 4, 7, 3, 3, 7, 5, 2, 2, 7, 5, 4, 12, 6, 9)
nll1 <- poisson_nll(x1)
nll2 <- poisson_nll(x2)

optimise(nll1, c(0, 100))$minimum

optimise(nll2, c(0, 100))$minimum

#Loops that should be left as is

#Modifying in place

trans <- list(
  disp = function(x) x * 0.0163871,
  am = function(x) factor(x, labels = c("auto", "manual"))
)
for(var in names(trans)) {
  mtcars[[var]] <- trans[[var]](mtcars[[var]])
}

#We wouldn't normally use lapply() to replace this loop directly, but it is possible. 
#Just replace the loop with lapply() by using <<-:
 
lapply(names(trans), function(var) {
  mtcars[[var]] <<- trans[[var]](mtcars[[var]])
})

#The for loop is gone, but the code is longer and much harder to understand. 
#The reader needs to understand <<- and how x[[y]] <<- z works (it's not simple!). 

#Recursive relationships
#For example, exponential smoothing works by taking a weighted average of the 
#current and previous data points. The exps() function below implements exponential 
#smoothing with a for loop.

exps <- function(x, alpha) {
  s <- numeric(length(x) + 1)
  for (i in seq_along(s)) {
    if (i == 1) {
      s[i] <- x[i]
    } else {
      s[i] <- alpha * x[i] + (1 - alpha) * s[i - 1]
    }
  }
  s
}
x <- runif(6)
exps(x, 0.5)

#A family of functions

#To finish off the chapter, this case study shows how you can use functionals 
#to take a simple building block and make it powerful and general. 

#We'll start by defining a very simple addition function, one which takes two scalar arguments:

add <- function(x, y) {
  stopifnot(length(x) == 1, length(y) == 1,
            is.numeric(x), is.numeric(y))
  x + y
}

#I'll also add an na.rm argument. A helper function will make this a bit 
#easier: if x is missing it should return y, if y is missing it should return x, 
#and if both x and y are missing then it should return another argument to the 
#function: identity. This function is probably a bit more general than what we need 
#now, but it's useful if we implement other binary operators.


rm_na <- function(x, y, identity) {
  if (is.na(x) && is.na(y)) {
    identity
  } else if (is.na(x)) {
    y
  } else {
    x
  }
}
rm_na(NA, 10, 0)

#This allows us to write a version of add() that can deal with missing values if needed:

add <- function(x, y, na.rm = FALSE) {
  if (na.rm && (is.na(x) || is.na(y))) rm_na(x, y, 0) else x + y
}
add(10, NA)

add(10, NA, na.rm = TRUE)
add(NA, NA)

add(NA, NA, na.rm = TRUE)

#Why did we pick an identity of 0? Why should add(NA, NA, na.rm = TRUE) return 0? Well, 
#for every other input it returns a number, so even if both arguments are NA, 
#it should still do that. What number should it return? We can figure it out because 
#addition is associative, which means that the order of addition doesn't matter. 
#That means that the following two function calls should return the same value:

add(add(3, NA, na.rm = TRUE), NA, na.rm = TRUE)

add(3, add(NA, NA, na.rm = TRUE), na.rm = TRUE)


#This implies that add(NA, NA, na.rm = TRUE) must be 0, and hence identity = 0 is the 
#correct default.

#Now that we have the basics working, we can extend the function to deal with more 
#complicated inputs. One obvious generalisation is to add more than two numbers. 
#We can do this by iteratively adding two numbers: if the input is c(1, 2, 3) 
#we compute add(add(1, 2), 3). This is a simple application of Reduce():
  
 
r_add <- function(xs, na.rm = TRUE) {
  Reduce(function(x, y) add(x, y, na.rm = na.rm), xs)
}
r_add(c(1, 4, 10))

#This looks good, but we need to test a few special cases:
r_add(NA, na.rm = TRUE)

r_add(numeric())

#The two problems are related. If we give Reduce() a length one vector, 
#it doesn't have anything to reduce, so it just returns the input. 
#If we give it an input of length zero, it always returns NULL. 
#The easiest way to fix this problem is to use the init argument of Reduce(). 
#This is added to the start of every input vector:


r_add <- function(xs, na.rm = TRUE) {
  Reduce(function(x, y) add(x, y, na.rm = na.rm), xs, init = 0)
}
r_add(c(1, 4, 10))

r_add(NA, na.rm = TRUE)

r_add(numeric())




fib <- function(n) {
  if (n < 2) return(1)
  a<-fib(n - 2) + fib(n - 1)
  return(a)
}

fib(4)



  

data<-data.frame(Year=c(1,1,1,1,2,2,2,2),Month=c(1,1,2,2))

data<-read.table('clipboard',header=T)

library(dplyr)

a<-data %>% mutate(rns=percent_rank(Value))

View(a)
